# -*- coding: utf-8 -*-
"""FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18BWF-YnFdtf9jqvJLccp6eLnLzLCXSg2
"""

# Install the SQLAlchemy library if it is not installed
!sudo apt-get install python3-dev libmysqlclient-dev > /dev/null
!pip install mysqlclient > /dev/null
!sudo pip3 install -U sql_magic > /dev/null
!pip install psycopg2-binary > /dev/null
!pip install yfinance
!pip install praw > /dev/null

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
!pip install pandas sklearn matplotlib
import pandas as pd 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib
import matplotlib.pyplot as plt
import requests
import json
import nltk
import time
import praw
import sklearn
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
# Make the graphs a bit prettier, and bigger
matplotlib.style.use(['seaborn-talk', 'seaborn-ticks', 'seaborn-whitegrid'])
plt.rcParams['figure.figsize'] = (15, 7)

from sqlalchemy import create_engine

conn_string = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(
    user="Unnamed", 
    password='RyMe/s67Jw4=', 
    host = 'jsedocc7.scrc.nyu.edu', 
    port=3306, 
    db='unnamed',
    encoding = 'utf-8'
)
engine = create_engine(conn_string)

engine.execute('CREATE DATABASE IF NOT EXISTS unnamed')
engine.execute('USE unnamed')

# Commented out IPython magic to ensure Python compatibility.
# Prepare sql_magic library that enable to query to database easily.
# %reload_ext sql_magic
# %config SQL.conn_name = 'engine'

def reddit_data(stock_nam, num_subm):
  reddit_list = []
  reddit = praw.Reddit(
      client_id="G35gFCI_dmA9VQ",
      client_secret="Dfb4wf9ZKSsfErunvyrlq5LxIaHQvA",
      user_agent="This is J's first!",
  )
  subreddit = reddit.subreddit("wallstreetbets")
  for submission in subreddit.search(stock_nam,limit=num_subm):
      reddit_list.append(submission.title)
  if len(reddit_list) == 0:
    return None
  else:
    return reddit_list

payload = ""
headers = {
  'Authorization': 'Bearer AAAAAAAAAAAAAAAAAAAAAOJVOAEAAAAAym0fsraKQD39gqqRzcG8n9mqffA%3D3MSxBpEwrMHl08PAAjNxIjy0AM8di31I9U7XXhIxT3WRgskHRM',
  'Cookie': 'guest_id=v1%3A161903049706096483; personalization_id="v1_4qcst6G485Yljpwma4VR3Q=="'
}
def search(query, start, end, max):
  starttime= str(start) + "T00:00:00Z" #format :yr-mm-dd
  endtime = str(end) + "T00:00:00Z" #format :yr-mm-dd, results are given in reverse chronological
  maxresults = str(max)
  url = "https://api.twitter.com/2/tweets/search/recent?query=" + query + "&start_time=" + starttime + "&end_time=" + endtime + "&max_results=" + maxresults
  response = requests.get(url, headers=headers, data=payload)
  tweets = response.json()
  data = []
  if "data" in tweets:
    for i in range(len(tweets['data'])):
      data.append(tweets['data'][i]['text'])
    return data
  else:
    return None

import yfinance as yf

def yfinance_data(stock_name , num_days):
  diff_list = []
  diff_avg = 0
  stock = yf.Ticker(stock_name)
  stock_df = stock.history(period="max").tail(num_days)
  for i in range(len(stock_df)):
    diff = stock_df["Close"][i] - stock_df["Open"][i]
    diff_avg += diff
    diff_list.append(diff)
  if len(diff_list) == 0:
    diff_avg = None
  else:
    diff_avg = diff_avg / len(diff_list)
  return diff_avg

def getSentiment(text):
  import requests
  import json
  try:
    sent = SentimentIntensityAnalyzer(text)
    return sent["compound"]
  except:
    endpoint = "https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze"
    username = "apikey"
    #password = "kOrWXuXr7PHdn6YHNAT4mjlWHuK_lRMgo2g0AEFSQJam"
    password = "yV0w0tQYFhAbgoJ0d-IMAtebL_5fdz-GRAvWzYb9bDa6"
    parameters = {
        'features': 'emotion,sentiment',
        'version' : '2018-11-16',
        'text': text,
        'language' : 'en',
    }
    resp = requests.get(endpoint, params=parameters, auth=(username, password))
    return resp.json()

stock_input = input("Enter a stock:")
start_time_input = input("Enter a start date, a date no further than 7 days from the present(yr-mm-dd):")
end_time_input = input("Enter a end date, a date not less than the start date or past the present(yr-mm-dd):")
amountOfTweetsAndSubmissions = int(input("Enter a number of tweets/submissions:"))
tweet_reddit_source_dict = {}
day_num = int(end_time_input[-1]) - int(start_time_input[-1])
tweet = search(stock_input, start_time_input, end_time_input, amountOfTweetsAndSubmissions)
redditSub = reddit_data(stock_input, amountOfTweetsAndSubmissions)
sent = []
stock_data = yfinance_data(stock_input, day_num)
sent_avg_list = []
sent_avg = 0
sent_avg_2 = 0
sent_2 = []
if tweet != None:
  for i in range(len(tweet)):
    sentiment = getSentiment(tweet[i])
    if isinstance(sentiment, int):
      sent.append(sentiment)
      sent_avg += sentiment
    else:
      sentiment_score = sentiment["sentiment"]["document"]["score"]
      sent.append(sentiment_score)
      sent_avg += sentiment_score
  sent_avg = sent_avg / len(sent)
sent_2 = []
sent_avg_redd = 0
if redditSub != None:
  for i in range(len(redditSub)):
    sentiment = getSentiment(redditSub[i])
    if isinstance(sentiment, int):
      sent_2.append(sentiment)
      sent_avg_redd += sentiment
    else:
      sentiment_score = sentiment["sentiment"]["document"]["score"]
      sent_2.append(sentiment_score)
      sent_avg_redd += sentiment_score
  sent_avg_redd = sent_avg_redd / len(sent_2)
total_avg = (sent_avg_redd + sent_avg) / 2
if redditSub != None and tweet != None:
  tweet.extend(redditSub)
tweet_reddit_source_dict["Twitter/Reddit Data"] = tweet
sent.extend(sent_2)
tweet_reddit_source_dict["Sentiment Compound Score"] = sent
tweetDataframe = pd.DataFrame(tweet_reddit_source_dict)
print("Predicted change in stock price:")
print(predictor(stock_input))
if total_avg > 0 and stock_data > 0:
  print("Positive Correlation") 
elif total_avg < 0 and stock_data < 0:
  print("Negative Correlation") 
else:
  print("No Correlation")
#Data Visualization
test_df = pd.read_sql("SELECT * FROM StockSentimentData", con=engine)
X_train, X_test, y_train, y_test = train_test_split(test_df["Sentiment"], test_df["StockAVGDifference"])
plt.scatter(X_train, y_train, label = 'Testing Data', color = "g", alpha = 0.7)
plt.scatter(X_train, y_train, label = 'Training Data', color = "r", alpha = 0.7)
plt.legend()
plt.title("Stock Sentiment vs. Stock Price AVG Difference")
plt.show()

LR = LinearRegression()
LR.fit(X_train.values.reshape(-1,1),y_train.values)

prediction = LR.predict(X_test.values.reshape(-1,1))

plt.plot(X_test, prediction, label = "Linear Regression" , color = 'b')
plt.scatter(X_test, y_test, label="Actual Test Data", color = 'g', alpha= .7)
plt.legend()
plt.show()
test_df.plot(x="Sentiment", y="StockAVGDifference", kind="scatter")

#Grabbing data from csv file
stocks_d = {"Stocks": [], "StockAVGDifference": [], "Sentiment" : [], "Correlation?" : []}
stocks = pd.read_csv('Top100.csv', encoding="utf-8", dtype="unicode")
stocks.drop(200, inplace=True)
stocks = stocks[:100]
stocks

#Code that gathers data of stocks and prepares a data frame for visualization | Don't run again
for i in range(len(stocks["Symbol"])):
  tweet_input = stocks["Symbol"][i]
  stock_input = stocks["Symbol"][i]
  reddit_input = '$' + stocks["Symbol"][i]
  start_time_input = "2021-05-08"
  end_time_input = "2021-05-12"
  amountOfTweetsReddit = 20
  tweet_source_dict = {}
  day_num = int(end_time_input[-1]) - int(start_time_input[-1]) 
  try:
    tweet = search(tweet_input, start_time_input, end_time_input, amountOfTweetsReddit)
  except:
    tweet = None
  try:
    reddit = reddit_data(reddit_input, amountOfTweetsReddit)
  except:
    reddit = None
  stock_data = yfinance_data(stock_input, day_num)
  ibm_sent = []
  ibm_sent_2 = []
  stock_sent_avg_dict = {}
  ibm_sent_avg_list = []
  ibm_sent_avg_2_list = []
  stock_avg_diff_list = []
  ibm_sent_avg = 0
  ibm_sent_avg_2 = 0
  total_avg = 0
  if tweet != None and reddit != None:
    for j in range(len(tweet)):
      sentiment = getSentiment(tweet[j])
      sentiment_score = sentiment["sentiment"]["document"]["score"]
      ibm_sent.append(sentiment_score)
      ibm_sent_avg += sentiment_score
    ibm_sent_avg = ibm_sent_avg / len(ibm_sent)
    for j in range(len(reddit)):
      sentiment = getSentiment(reddit[j])
      sentiment_score = sentiment["sentiment"]["document"]["score"]
      ibm_sent_2.append(sentiment_score)
      ibm_sent_avg_2 += sentiment_score
    ibm_sent_avg_2 = ibm_sent_avg_2 / len(ibm_sent_2)
    total_avg = (ibm_sent_avg + ibm_sent_avg_2) / 2
    stocks_d["Sentiment"].append(total_avg)
    stocks_d["StockAVGDifference"].append(stock_data)
    stocks_d["Stocks"].append(stocks["Symbol"][i])  
  elif tweet != None or reddit != None: 
    if tweet != None:
      for j in range(len(tweet)):
        sentiment = getSentiment(tweet[j])
        sentiment_score = sentiment["sentiment"]["document"]["score"]
        ibm_sent.append(sentiment_score)
        ibm_sent_avg += sentiment_score
      ibm_sent_avg = ibm_sent_avg / len(ibm_sent)
      stocks_d["Sentiment"].append(ibm_sent_avg)
      stocks_d["StockAVGDifference"].append(stock_data)
      stocks_d["Stocks"].append(stocks["Symbol"][i])
    elif reddit != None:
      for j in range(len(reddit)):
        sentiment = getSentiment(reddit[j])
        sentiment_score = sentiment["sentiment"]["document"]["score"]
        ibm_sent_2.append(sentiment_score)
        ibm_sent_avg_2 += sentiment_score
      ibm_sent_avg_2 = ibm_sent_avg_2 / len(ibm_sent_2)
      stocks_d["Sentiment"].append(ibm_sent_avg_2) 
      stocks_d["StockAVGDifference"].append(stock_data)
      stocks_d["Stocks"].append(stocks["Symbol"][i])  
  else:
      stocks_d["Sentiment"].append(None) 
      stocks_d["StockAVGDifference"].append(stock_data)
      stocks_d["Stocks"].append(stocks["Symbol"][i]) 
for i in range(len(stocks_d["Sentiment"])):
  if stocks_d["Sentiment"][i] != None and stocks_d["StockAVGDifference"][i] != None:
    if stocks_d["Sentiment"][i]>=0 and stocks_d["StockAVGDifference"][i]>=0 or stocks_d["Sentiment"][i]<=0 and stocks_d["StockAVGDifference"][i]<=0:
      stocks_d["Correlation?"].append("Yes")
    else:
      stocks_d["Correlation?"].append("No")
  else:
    stocks_d["Correlation?"].append("No Data")
print(len(stocks_d["Stocks"]))
print(len(stocks_d["Sentiment"]))
print(len(stocks_d["Correlation?"]))
stocks_df = pd.DataFrame(stocks_d)
stocks_df

#Drop stock causing errors
#stocks_df.drop(69 , inplace=True)
#Send 100 most popular stock data into SQL
#stocks_df.to_sql("StockSentimentData", con=engine,index = False, chunksize=1000)

test_df.plot(x="Sentiment", y="StockAVGDifference", kind="scatter")
#pd.plotting.scatter_matrix(test_df[['Stocks','Sentiment','StockAVGDifference', 'Correlation?']], c = pd.Categorical(test_df["Sentiment"]).codes, cmap = 'bwr', figsize = (20,20))
#pd.plotting.scatter_matrix(test_df[['Stocks','Sentiment','StockAVGDifference', 'Correlation?']], c = pd.Categorical(test_df["Correlation?"]).codes, cmap = 'bwr', figsize = (20,20))

def predictor(stock):
  stockdf = pd.read_sql("SELECT * FROM StockSentimentData", con=engine)
  stock_names = stockdf["Stocks"].tolist()
  predicted_stock_change = 0
  if stock in stock_names:
    loc = stock_names.index(stock)
  else:
    message = "Stock Not Available"
    return message
  X = stockdf[["Sentiment"]]
  Y = stockdf[["StockAVGDifference"]]
  reg = LinearRegression()
  reg.fit(Y, X)
  price_diff_pred = reg.predict(X)
  predicted_stock_change = price_diff_pred[loc][0]
  return predicted_stock_change

print(predictor("UAL"))

